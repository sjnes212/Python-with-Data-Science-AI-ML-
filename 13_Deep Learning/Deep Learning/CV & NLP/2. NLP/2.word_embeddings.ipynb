{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><br>One Hot Encoding</h1></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: gensim in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\silwa\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\silwa\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\silwa\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\silwa\\.conda\\envs\\daks\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'hey how are you doing? you have a good day good day'\n",
    "\n",
    "# word tokenize\n",
    "word = ['hey','how','are','you','doing','?','you','have','a','good','day']\n",
    "# stop words remove\n",
    "word = ['hey','doing','?','good','day','good','day']\n",
    "# unique words\n",
    "word = ['hey','doing','?','good','day']\n",
    "\n",
    "encoding = {'hey':0,'doing':1,'?':2,'good':3,'day':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hey it is a good day'\n",
    "[0,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = ['road','sweet','computer','AI']\n",
    "numbers = {'road':1, 'sweet':2, 'computer':3, 'AI':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'road': 1, 'sweet': 2, 'computer': 3, 'AI': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vocab\n",
      "0    hey\n",
      "1  doing\n",
      "2      ?\n",
      "3   good\n",
      "4    day\n",
      "5   word\n",
      "6  nepal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create DataFrame\n",
    "df = pd.DataFrame({'vocab': ['hey','doing','?','good','day','word','nepal']})\n",
    "\n",
    "#view DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vocab    0    1    2    3    4    5    6\n",
      "0    hey  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
      "1  doing  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
      "2      ?  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "3   good  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
      "4    day  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
      "5   word  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
      "6  nepal  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#creating instance of one-hot-encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#perform one-hot encoding on 'team' column \n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(df[['vocab']]).toarray())\n",
    "\n",
    "#merge one-hot encoded columns back with original DataFrame\n",
    "final_df = df.join(encoder_df)\n",
    "\n",
    "#view final df\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab : 5\n",
    "hey = [0,0,0,0,1]\n",
    "doing = [0,0,1,0,0]\n",
    "vector = (1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hey nepal is good'\n",
    "encoding = [0,6,3]\n",
    "one_hot = [[0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0],[0,0,0,1,0,0,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating frequency distribution of words using nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Achievers achievers achievers are not afraid of Challenges, rather they relish them, thrive in them, use them.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences from the text corpus\n",
    "tokenized_text=word_tokenize(text)#using CountVectorizer and removing stopwords in english language\n",
    "cv1= CountVectorizer(lowercase=True,stop_words='english')#fitting the tonized senetnecs to the countvectorizer\n",
    "text_counts=cv1.fit_transform(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'achievers': 0, 'afraid': 1, 'challenges': 2, 'relish': 3, 'thrive': 4, 'use': 5}\n",
      "['achievers', 'afraid', 'challenges', 'relish', 'thrive', 'use']\n",
      "[[1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing the vocabulary and the frequency distribution pf vocabulary in tokinzed sentences\n",
    "print(cv1.vocabulary_)\n",
    "print(sorted(cv1.vocabulary_))\n",
    "print(text_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Achievers achievers achievers are not afraid of Challenges, rather they relish them, thrive in them, use them.\n",
    "        Challenges makes is stronger.\n",
    "        Challenges makes us uncomfortable. \n",
    "        If you get comfortable with uncomfort then you will grow. \n",
    "        I hate the hate that so I hate.\n",
    "        Challenge the challenge the challenge\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences from the text corpus\n",
    "tokenized_text=sent_tokenize(text)#using CountVectorizer and removing stopwords in english language\n",
    "cv1= CountVectorizer(lowercase=True,stop_words='english')#fitting the tonized senetnecs to the countvectorizer\n",
    "text_counts=cv1.fit_transform(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'achievers': 0, 'afraid': 1, 'challenges': 3, 'relish': 8, 'thrive': 10, 'use': 13, 'makes': 7, 'stronger': 9, 'uncomfortable': 12, 'comfortable': 4, 'uncomfort': 11, 'grow': 5, 'hate': 6, 'challenge': 2}\n",
      "['achievers', 'afraid', 'challenge', 'challenges', 'comfortable', 'grow', 'hate', 'makes', 'relish', 'stronger', 'thrive', 'uncomfort', 'uncomfortable', 'use']\n",
      "[[3 1 0 1 0 0 0 0 1 0 1 0 0 1]\n",
      " [0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 3 0 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# printing the vocabulary and the frequency distribution pf vocabulary in tokinzed sentences\n",
    "print(cv1.vocabulary_)\n",
    "print(sorted(cv1.vocabulary_))\n",
    "print(text_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 3\n",
      "afraid : 1\n",
      "challenges : 1\n",
      "relish : 1\n",
      "thrive : 1\n",
      "use : 1\n",
      "makes : 0\n",
      "stronger : 0\n",
      "uncomfortable : 0\n",
      "comfortable : 0\n",
      "uncomfort : 0\n",
      "grow : 0\n",
      "hate : 0\n",
      "challenge : 0\n",
      "For 2 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 0\n",
      "afraid : 0\n",
      "challenges : 1\n",
      "relish : 0\n",
      "thrive : 0\n",
      "use : 0\n",
      "makes : 1\n",
      "stronger : 1\n",
      "uncomfortable : 0\n",
      "comfortable : 0\n",
      "uncomfort : 0\n",
      "grow : 0\n",
      "hate : 0\n",
      "challenge : 0\n",
      "For 3 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 0\n",
      "afraid : 0\n",
      "challenges : 1\n",
      "relish : 0\n",
      "thrive : 0\n",
      "use : 0\n",
      "makes : 1\n",
      "stronger : 0\n",
      "uncomfortable : 1\n",
      "comfortable : 0\n",
      "uncomfort : 0\n",
      "grow : 0\n",
      "hate : 0\n",
      "challenge : 0\n",
      "For 4 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 0\n",
      "afraid : 0\n",
      "challenges : 0\n",
      "relish : 0\n",
      "thrive : 0\n",
      "use : 0\n",
      "makes : 0\n",
      "stronger : 0\n",
      "uncomfortable : 0\n",
      "comfortable : 1\n",
      "uncomfort : 1\n",
      "grow : 1\n",
      "hate : 0\n",
      "challenge : 0\n",
      "For 5 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 0\n",
      "afraid : 0\n",
      "challenges : 0\n",
      "relish : 0\n",
      "thrive : 0\n",
      "use : 0\n",
      "makes : 0\n",
      "stronger : 0\n",
      "uncomfortable : 0\n",
      "comfortable : 0\n",
      "uncomfort : 0\n",
      "grow : 0\n",
      "hate : 3\n",
      "challenge : 0\n",
      "For 6 sentence\n",
      ">>>>>>>>>>>>>>>>>\n",
      "achievers : 0\n",
      "afraid : 0\n",
      "challenges : 0\n",
      "relish : 0\n",
      "thrive : 0\n",
      "use : 0\n",
      "makes : 0\n",
      "stronger : 0\n",
      "uncomfortable : 0\n",
      "comfortable : 0\n",
      "uncomfort : 0\n",
      "grow : 0\n",
      "hate : 0\n",
      "challenge : 3\n"
     ]
    }
   ],
   "source": [
    "for index,sentence in enumerate(text_counts.toarray()):\n",
    "    print(f\"For {index+1} sentence\")\n",
    "    print(\">>>>>>>>>>>>>>>>>\")\n",
    "    for keys,items in cv1.vocabulary_.items():\n",
    "        print(f\"{keys} : {sentence[items]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><br>TF-IDF (Term Frequency-Inverse Document Frequency)</h1></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency (TF)\n",
    "The Term Frequency (TF) is a measure of how frequently a term \n",
    "ð‘¡\n",
    "t appears in a document \n",
    "ð‘‘\n",
    "d. It can be calculated using the following formula:\n",
    "\n",
    "TF\n",
    "(\n",
    "ð‘¡\n",
    ",\n",
    "ð‘‘\n",
    ")\n",
    "=\n",
    "ð‘“\n",
    "ð‘¡\n",
    ",\n",
    "ð‘‘\n",
    "ð‘\n",
    "ð‘‘\n",
    "TF(t,d)= \n",
    "N \n",
    "d\n",
    "â€‹\n",
    " \n",
    "f \n",
    "t,d\n",
    "â€‹\n",
    " \n",
    "â€‹\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "ð‘“\n",
    "ð‘¡\n",
    ",\n",
    "ð‘‘\n",
    "f \n",
    "t,d\n",
    "â€‹\n",
    "  is the number of times term \n",
    "ð‘¡\n",
    "t appears in document \n",
    "ð‘‘\n",
    "d.\n",
    "ð‘\n",
    "ð‘‘\n",
    "N \n",
    "d\n",
    "â€‹\n",
    "  is the total number of terms in document \n",
    "ð‘‘\n",
    "d.\n",
    "Inverse Document Frequency (IDF)\n",
    "The Inverse Document Frequency (IDF) is a measure of how important a term \n",
    "ð‘¡\n",
    "t is across the entire corpus. It is calculated as follows:\n",
    "\n",
    "IDF\n",
    "(\n",
    "ð‘¡\n",
    ")\n",
    "=\n",
    "log\n",
    "â¡\n",
    "(\n",
    "ð‘\n",
    "1\n",
    "+\n",
    "ð‘›\n",
    "ð‘¡\n",
    ")\n",
    "+\n",
    "1\n",
    "IDF(t)=log( \n",
    "1+n \n",
    "t\n",
    "â€‹\n",
    " \n",
    "N\n",
    "â€‹\n",
    " )+1\n",
    "\n",
    "where:\n",
    "\n",
    "ð‘\n",
    "N is the total number of documents in the corpus.\n",
    "ð‘›\n",
    "ð‘¡\n",
    "n \n",
    "t\n",
    "â€‹\n",
    "  is the number of documents containing the term \n",
    "ð‘¡\n",
    "t.\n",
    "The addition of 1 in the denominator is used to prevent division by zero (in case a term does not appear in any document).\n",
    "\n",
    "TF-IDF Calculation\n",
    "The TF-IDF value for a term \n",
    "ð‘¡\n",
    "t in a document \n",
    "ð‘‘\n",
    "d is then calculated by multiplying the TF and IDF values:\n",
    "\n",
    "TF-IDF\n",
    "(\n",
    "ð‘¡\n",
    ",\n",
    "ð‘‘\n",
    ")\n",
    "=\n",
    "TF\n",
    "(\n",
    "ð‘¡\n",
    ",\n",
    "ð‘‘\n",
    ")\n",
    "Ã—\n",
    "IDF\n",
    "(\n",
    "ð‘¡\n",
    ")\n",
    "TF-IDF(t,d)=TF(t,d)Ã—IDF(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 12, 'cycle': 1, 'is': 6, 'ridden': 9, 'on': 8, 'track': 13, 'bus': 0, 'he': 5, 'driven': 2, 'road': 10, 'driving': 3, 'my': 7, 'favourate': 4, 'sport': 11}\n",
      "['bus', 'cycle', 'driven', 'driving', 'favourate', 'he', 'is', 'my', 'on', 'ridden', 'road', 'sport', 'the', 'track']\n",
      "[1.51082562 1.51082562 1.91629073 1.91629073 1.91629073 1.51082562\n",
      " 1.         1.91629073 1.51082562 1.91629073 1.91629073 1.91629073\n",
      " 1.22314355 1.91629073]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The cycle is ridden on the track.\", \n",
    "\t\"The bus he is driven on the road.\",\n",
    "\t\"He  is driving the bus.\",\n",
    "    \"Cycle is my favourate sport.\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(sorted(vectorizer.vocabulary_))\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : 1.2231435513142097\n",
      "cycle : 1.5108256237659907\n",
      "is : 1.0\n",
      "ridden : 1.916290731874155\n",
      "on : 1.5108256237659907\n",
      "track : 1.916290731874155\n",
      "bus : 1.5108256237659907\n",
      "he : 1.5108256237659907\n",
      "driven : 1.916290731874155\n",
      "road : 1.916290731874155\n",
      "driving : 1.916290731874155\n",
      "my : 1.916290731874155\n",
      "favourate : 1.916290731874155\n",
      "sport : 1.916290731874155\n"
     ]
    }
   ],
   "source": [
    "for keys,items in vectorizer.vocabulary_.items():\n",
    "    print(f\"{keys} : {vectorizer.idf_[items]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"The cycle is ridden on the track.\", => 5\\n\"The bus he is driven on the road.\", => 2\\n\"He  is driving the bus.\", =>10\\n\"Cycle is my favourate sport.\"=>15\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\"The cycle is ridden on the track.\", => 5\n",
    "\"The bus he is driven on the road.\", => 2\n",
    "\"He  is driving the bus.\", =>10\n",
    "\"Cycle is my favourate sport.\"=>15\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    " \n",
    "for model_name in list(gensim.downloader.info()['models'].keys()):\n",
    "  print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    " \n",
    "google_news_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07080078,  0.09472656,  0.10302734,  0.17285156, -0.13378906,\n",
       "       -0.07080078,  0.06933594, -0.21875   , -0.11230469,  0.06225586,\n",
       "       -0.05639648, -0.26171875, -0.21289062,  0.1328125 , -0.13574219,\n",
       "       -0.09472656,  0.06982422,  0.09960938, -0.06494141, -0.16601562,\n",
       "        0.0072937 , -0.11376953,  0.23339844, -0.12158203, -0.03271484,\n",
       "        0.03369141, -0.31054688, -0.02990723,  0.0456543 , -0.08447266,\n",
       "        0.13769531, -0.15039062, -0.22460938, -0.01519775, -0.19726562,\n",
       "       -0.04663086, -0.19824219, -0.00279236,  0.14257812,  0.03735352,\n",
       "        0.03442383, -0.078125  ,  0.11376953,  0.109375  ,  0.14160156,\n",
       "       -0.10253906,  0.05981445, -0.10742188, -0.14257812, -0.01708984,\n",
       "       -0.11914062,  0.00335693,  0.24511719,  0.09326172, -0.22851562,\n",
       "        0.02099609, -0.12792969, -0.04077148,  0.06176758,  0.01190186,\n",
       "        0.02148438,  0.0255127 ,  0.01000977,  0.04370117, -0.07910156,\n",
       "       -0.19433594,  0.16503906, -0.10888672,  0.08056641,  0.18261719,\n",
       "       -0.03015137,  0.18652344, -0.0100708 ,  0.00189972,  0.03112793,\n",
       "       -0.1875    , -0.05859375, -0.01037598,  0.10302734, -0.07177734,\n",
       "       -0.17285156,  0.17480469,  0.234375  , -0.10498047,  0.10986328,\n",
       "        0.05883789,  0.00430298, -0.13671875,  0.09619141,  0.11230469,\n",
       "       -0.03491211, -0.08740234, -0.16992188, -0.08398438, -0.09814453,\n",
       "        0.03930664,  0.19335938, -0.09960938,  0.33984375, -0.06201172,\n",
       "       -0.18847656, -0.03173828,  0.08642578,  0.01049805, -0.03393555,\n",
       "        0.1484375 , -0.12988281,  0.05981445,  0.15722656,  0.05957031,\n",
       "       -0.01989746, -0.1171875 , -0.02331543, -0.06347656,  0.02856445,\n",
       "       -0.03588867, -0.04101562, -0.2578125 ,  0.12597656,  0.00952148,\n",
       "        0.125     ,  0.13085938, -0.15722656,  0.01806641, -0.09082031,\n",
       "        0.24511719, -0.21875   ,  0.07128906,  0.07519531, -0.03710938,\n",
       "       -0.04785156,  0.09179688,  0.16992188,  0.20019531, -0.19921875,\n",
       "       -0.11621094, -0.19238281, -0.00366211,  0.12695312,  0.14160156,\n",
       "        0.19140625, -0.12988281,  0.14355469, -0.02246094, -0.03039551,\n",
       "        0.00613403, -0.0378418 , -0.125     ,  0.09716797, -0.09277344,\n",
       "        0.34570312, -0.17871094, -0.21679688, -0.14648438,  0.03063965,\n",
       "       -0.33007812, -0.16503906,  0.07080078,  0.08544922, -0.17871094,\n",
       "       -0.02844238, -0.01379395, -0.04907227, -0.10498047, -0.05957031,\n",
       "       -0.2890625 ,  0.02453613, -0.02209473, -0.1953125 ,  0.09619141,\n",
       "       -0.20214844, -0.04443359,  0.1015625 ,  0.0246582 ,  0.06787109,\n",
       "        0.02770996,  0.23242188, -0.19335938,  0.10644531, -0.28125   ,\n",
       "        0.0168457 ,  0.01660156,  0.01550293, -0.00765991,  0.03088379,\n",
       "        0.13183594,  0.13867188,  0.0402832 ,  0.18164062,  0.06542969,\n",
       "        0.01397705,  0.01647949, -0.27929688,  0.109375  ,  0.359375  ,\n",
       "        0.05395508, -0.00376892,  0.11914062, -0.10058594, -0.08642578,\n",
       "        0.04394531, -0.00854492, -0.02490234, -0.14453125, -0.00376892,\n",
       "        0.03979492,  0.02380371, -0.17285156, -0.16894531,  0.13964844,\n",
       "        0.06591797,  0.04736328, -0.14941406, -0.23339844, -0.18164062,\n",
       "        0.07714844,  0.17480469,  0.05151367, -0.25      ,  0.03808594,\n",
       "        0.06689453, -0.14746094,  0.11181641,  0.2734375 ,  0.05810547,\n",
       "        0.03173828, -0.01159668,  0.14941406,  0.10498047,  0.12792969,\n",
       "       -0.03442383, -0.08251953,  0.12988281,  0.02856445,  0.18457031,\n",
       "        0.05273438,  0.1640625 ,  0.09814453,  0.11669922,  0.21582031,\n",
       "        0.11767578,  0.13867188,  0.17675781,  0.2578125 , -0.20996094,\n",
       "        0.0625    , -0.23339844, -0.01403809, -0.02941895, -0.1796875 ,\n",
       "        0.15722656,  0.03808594,  0.23046875,  0.27929688,  0.08154297,\n",
       "       -0.09960938, -0.0043335 , -0.05297852,  0.09375   , -0.19335938,\n",
       "       -0.00099182,  0.17773438,  0.14257812, -0.03564453, -0.05419922,\n",
       "        0.11230469, -0.006073  , -0.01544189,  0.01745605, -0.04443359,\n",
       "       -0.15527344, -0.05615234,  0.04052734,  0.12011719,  0.15332031,\n",
       "       -0.05273438,  0.11816406,  0.0123291 , -0.27148438,  0.23339844,\n",
       "       -0.1640625 ,  0.12353516,  0.08300781, -0.12255859,  0.25390625,\n",
       "       -0.1953125 , -0.10693359,  0.125     , -0.06884766, -0.04541016,\n",
       "       -0.01586914,  0.03149414,  0.15820312, -0.06835938, -0.13378906,\n",
       "        0.00236511, -0.28710938,  0.05078125,  0.05126953,  0.34375   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors['nepal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Capital of Britain: (Paris - France) + Nepal\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Finding Capital of Britain given Capital of France: (Paris - France) + Britain = \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding Capital of Britain: (Paris - France) + Nepal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m capital \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_news_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNepal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFrance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(capital)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\silwa\\.conda\\envs\\daks\\lib\\site-packages\\gensim\\models\\keyedvectors.py:823\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    820\u001b[0m positive \u001b[38;5;241m=\u001b[39m _ensure_list(positive)\n\u001b[0;32m    821\u001b[0m negative \u001b[38;5;241m=\u001b[39m _ensure_list(negative)\n\u001b[1;32m--> 823\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_norms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m clip_end \u001b[38;5;241m=\u001b[39m clip_end \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors)\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m restrict_vocab:\n",
      "File \u001b[1;32mc:\\Users\\silwa\\.conda\\envs\\daks\\lib\\site-packages\\gensim\\models\\keyedvectors.py:708\u001b[0m, in \u001b[0;36mKeyedVectors.fill_norms\u001b[1;34m(self, force)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03mEnsure per-vector norms are available.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    705\u001b[0m \n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m force:\n\u001b[1;32m--> 708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\linalg\\linalg.py:2541\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mabs\u001b[39m(x), axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[1;32m-> 2541\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\u001b[38;5;241m.\u001b[39mreal\n\u001b[0;32m   2542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add\u001b[38;5;241m.\u001b[39mreduce(s, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims))\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[0;32m   2544\u001b[0m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "# Finding Capital of Britain given Capital of France: (Paris - France) + Britain = \n",
    "print(\"Finding Capital of Britain: (Paris - France) + Nepal\")\n",
    "capital = google_news_vectors.most_similar([\"Paris\", \"Nepal\"], [\"France\"], topn=1)\n",
    "print(capital)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Capital of India: (Berlin - Germany) + India\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Finding Capital of India given Capital of Germany: (Berlin - Germany) + India = \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding Capital of India: (Berlin - Germany) + India\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m capital \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_news_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBerlin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIndia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGermany\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(capital)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\silwa\\.conda\\envs\\daks\\lib\\site-packages\\gensim\\models\\keyedvectors.py:823\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    820\u001b[0m positive \u001b[38;5;241m=\u001b[39m _ensure_list(positive)\n\u001b[0;32m    821\u001b[0m negative \u001b[38;5;241m=\u001b[39m _ensure_list(negative)\n\u001b[1;32m--> 823\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_norms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m clip_end \u001b[38;5;241m=\u001b[39m clip_end \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors)\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m restrict_vocab:\n",
      "File \u001b[1;32mc:\\Users\\silwa\\.conda\\envs\\daks\\lib\\site-packages\\gensim\\models\\keyedvectors.py:708\u001b[0m, in \u001b[0;36mKeyedVectors.fill_norms\u001b[1;34m(self, force)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03mEnsure per-vector norms are available.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    705\u001b[0m \n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m force:\n\u001b[1;32m--> 708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\linalg\\linalg.py:2541\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mabs\u001b[39m(x), axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[1;32m-> 2541\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\u001b[38;5;241m.\u001b[39mreal\n\u001b[0;32m   2542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add\u001b[38;5;241m.\u001b[39mreduce(s, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims))\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[0;32m   2544\u001b[0m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "# Finding Capital of India given Capital of Germany: (Berlin - Germany) + India = \n",
    "print(\"Finding Capital of India: (Berlin - Germany) + India\")\n",
    "capital = google_news_vectors.most_similar([\"Berlin\", \"India\"], [\"Germany\"], topn=1)\n",
    "print(capital)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 similar words to BMW:\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Finding words similar to BMW\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 similar words to BMW:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_news_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBMW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(word)\n",
      "File \u001b[1;32mc:\\Users\\silwa\\.conda\\envs\\daks\\lib\\site-packages\\gensim\\models\\keyedvectors.py:823\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    820\u001b[0m positive \u001b[38;5;241m=\u001b[39m _ensure_list(positive)\n\u001b[0;32m    821\u001b[0m negative \u001b[38;5;241m=\u001b[39m _ensure_list(negative)\n\u001b[1;32m--> 823\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_norms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m clip_end \u001b[38;5;241m=\u001b[39m clip_end \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors)\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m restrict_vocab:\n",
      "File \u001b[1;32mc:\\Users\\silwa\\.conda\\envs\\daks\\lib\\site-packages\\gensim\\models\\keyedvectors.py:708\u001b[0m, in \u001b[0;36mKeyedVectors.fill_norms\u001b[1;34m(self, force)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03mEnsure per-vector norms are available.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    705\u001b[0m \n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m force:\n\u001b[1;32m--> 708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\linalg\\linalg.py:2541\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mabs\u001b[39m(x), axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[1;32m-> 2541\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\u001b[38;5;241m.\u001b[39mreal\n\u001b[0;32m   2542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add\u001b[38;5;241m.\u001b[39mreduce(s, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims))\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[0;32m   2544\u001b[0m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "# Finding words similar to BMW\n",
    "print(\"5 similar words to BMW:\")\n",
    "words = google_news_vectors.most_similar(\"BMW\", topn=5)\n",
    "for word in words:\n",
    "    print(word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 similar words to beautiful:\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Finding words similar to Beautiful\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3 similar words to beautiful:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_news_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeautiful\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m      5\u001b[0m       \u001b[38;5;28mprint\u001b[39m(word)\n",
      "File \u001b[1;32mc:\\Users\\silwa\\.conda\\envs\\daks\\lib\\site-packages\\gensim\\models\\keyedvectors.py:823\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    820\u001b[0m positive \u001b[38;5;241m=\u001b[39m _ensure_list(positive)\n\u001b[0;32m    821\u001b[0m negative \u001b[38;5;241m=\u001b[39m _ensure_list(negative)\n\u001b[1;32m--> 823\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_norms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m clip_end \u001b[38;5;241m=\u001b[39m clip_end \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors)\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m restrict_vocab:\n",
      "File \u001b[1;32mc:\\Users\\silwa\\.conda\\envs\\daks\\lib\\site-packages\\gensim\\models\\keyedvectors.py:708\u001b[0m, in \u001b[0;36mKeyedVectors.fill_norms\u001b[1;34m(self, force)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03mEnsure per-vector norms are available.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    705\u001b[0m \n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m force:\n\u001b[1;32m--> 708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\linalg\\linalg.py:2541\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mabs\u001b[39m(x), axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[1;32m-> 2541\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\u001b[38;5;241m.\u001b[39mreal\n\u001b[0;32m   2542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(add\u001b[38;5;241m.\u001b[39mreduce(s, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims))\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[0;32m   2544\u001b[0m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "# Finding words similar to Beautiful\n",
    "print(\"3 similar words to beautiful:\")\n",
    "words = google_news_vectors.most_similar(\"beautiful\", topn=3)\n",
    "for word in words:\n",
    "      print(word)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between fight and battle: 0.7021284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding cosine similarity between fight and battle\n",
    "cosine = google_news_vectors.similarity(\"fight\", \"battle\")\n",
    "print(\"Cosine similarity between fight and battle:\", cosine)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between fight and love: 0.13506128\n"
     ]
    }
   ],
   "source": [
    "# Finding cosine similarity between fight and love\n",
    "cosine = google_news_vectors.similarity(\"fight\", \"love\")\n",
    "print(\"Cosine similarity between fight and love:\", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><br>GloVe</h1><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    " \n",
    "for model_name in list(gensim.downloader.info()['models'].keys()):\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    " \n",
    "google_news_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eating', 0.8706035017967224), ('ate', 0.8597103357315063), ('eaten', 0.8458368182182312), ('eats', 0.7801411747932434), ('meal', 0.7678447365760803), ('chicken', 0.7672050595283508), ('meat', 0.7620247602462769), ('cooked', 0.7545220851898193), ('fish', 0.7533289790153503), ('drink', 0.7511290907859802)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding similar words\n",
    "similar = google_news_vectors.most_similar(\"eat\", topn=10)\n",
    "print(similar)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = google_news_vectors[\"eat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.4295e-01, -4.2946e-01, -5.4277e-01, -1.0307e+00,  1.2056e+00,\n",
       "       -2.7174e-01, -6.3561e-01, -1.5065e-02,  3.7856e-01,  4.6474e-02,\n",
       "       -1.3102e-01,  6.0500e-01,  1.6391e+00,  2.3940e-01,  1.2128e+00,\n",
       "        8.3178e-01,  7.3893e-01,  1.5200e-01, -1.4175e-01, -8.8384e-01,\n",
       "        2.0829e-02, -3.2545e-01,  1.8035e+00,  1.0045e+00,  5.8484e-01,\n",
       "       -6.2031e-01, -4.3296e-01,  2.3562e-01,  1.3027e+00, -8.1264e-01,\n",
       "        2.3158e+00,  1.1030e+00, -6.0608e-01,  1.0101e+00, -2.2426e-01,\n",
       "        1.8908e-02, -1.0931e-01,  3.8350e-01,  7.7362e-01, -8.1927e-02,\n",
       "       -3.4040e-01, -1.5143e-03, -5.6640e-02,  8.7359e-01,  1.4805e+00,\n",
       "        6.9421e-01, -3.0966e-01, -9.0826e-01,  3.7277e-03,  8.4550e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.20",
   "language": "python",
   "name": "daks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
